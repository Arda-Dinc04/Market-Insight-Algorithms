{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Open      High      Low     Close     Volume  Ex-Dividend  Split Ratio  \\\n",
      "0    1.8000    1.8900    1.700    1.8492    72531.0          0.0          1.0   \n",
      "1   10.6900   10.8600   10.451   10.5300   557983.0          0.0          1.0   \n",
      "2  140.5000  140.5700  140.500  140.5200  1189653.0          0.0          1.0   \n",
      "3    3.4672    3.4672    3.350    3.3900     1925.0          0.0          1.0   \n",
      "4    3.0900    3.2000    2.970    3.1100  1222100.0          0.0          1.0   \n",
      "5    2.3600    2.4390    2.220    2.2600  2135214.0          0.0          1.0   \n",
      "6    2.8000    2.8300    2.670    2.7100   120485.0          0.0          1.0   \n",
      "\n",
      "   Adj. Open  Adj. High  Adj. Low  Adj. Close  Adj. Volume Ticker  \n",
      "0     1.8000     1.8900     1.700      1.8492      72531.0   BPTH  \n",
      "1    10.6900    10.8600    10.451     10.5300     557983.0   SNCR  \n",
      "2   140.5000   140.5700   140.500    140.5200    1189653.0    PRE  \n",
      "3     3.4672     3.4672     3.350      3.3900       1925.0   MIND  \n",
      "4     3.0900     3.2000     2.970      3.1100    1222100.0   AMPE  \n",
      "5     2.3600     2.4390     2.220      2.2600    2135214.0   XXII  \n",
      "6     2.8000     2.8300     2.670      2.7100     120485.0   SEAC  \n",
      "       Open      High      Low     Close     Volume  Ex-Dividend  Split Ratio  \\\n",
      "0    1.8000    1.8900    1.700    1.8492    72531.0          0.0          1.0   \n",
      "1   10.6900   10.8600   10.451   10.5300   557983.0          0.0          1.0   \n",
      "2  140.5000  140.5700  140.500  140.5200  1189653.0          0.0          1.0   \n",
      "3    3.4672    3.4672    3.350    3.3900     1925.0          0.0          1.0   \n",
      "4    3.0900    3.2000    2.970    3.1100  1222100.0          0.0          1.0   \n",
      "5    2.3600    2.4390    2.220    2.2600  2135214.0          0.0          1.0   \n",
      "6    2.8000    2.8300    2.670    2.7100   120485.0          0.0          1.0   \n",
      "\n",
      "   Adj. Open  Adj. High  Adj. Low  Adj. Close  Adj. Volume Ticker  \n",
      "0     1.8000     1.8900     1.700      1.8492      72531.0   BPTH  \n",
      "1    10.6900    10.8600    10.451     10.5300     557983.0   SNCR  \n",
      "2   140.5000   140.5700   140.500    140.5200    1189653.0    PRE  \n",
      "3     3.4672     3.4672     3.350      3.3900       1925.0   MIND  \n",
      "4     3.0900     3.2000     2.970      3.1100    1222100.0   AMPE  \n",
      "5     2.3600     2.4390     2.220      2.2600    2135214.0   XXII  \n",
      "6     2.8000     2.8300     2.670      2.7100     120485.0   SEAC  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import quandl\n",
    "\n",
    "# Install the quandl package if it's not already installed\n",
    "# !pip install quandl\n",
    "\n",
    "# Set the API key\n",
    "quandl.ApiConfig.api_key = \"xyrEL2EBKf8XvGzp1YdA\"\n",
    "\n",
    "# Load the CSV file\n",
    "robinhood_profit_df = pd.read_csv('Robinhood_Profit.csv')\n",
    "\n",
    "# Extract the unique tickers from the CSV file\n",
    "tickers = robinhood_profit_df['Instrument'].unique()\n",
    "\n",
    "# Function to check if a ticker is valid\n",
    "def is_valid_ticker(ticker):\n",
    "    try:\n",
    "        # Try fetching a single row of data to validate the ticker\n",
    "        quandl.get(f\"WIKI/{ticker}\", rows=1)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Filter out invalid tickers\n",
    "valid_tickers = [ticker for ticker in tickers if is_valid_ticker(ticker)]\n",
    "\n",
    "# Prepare an empty DataFrame to store the data\n",
    "data_frames = []\n",
    "\n",
    "# Fetch the data for each valid ticker and store it in the DataFrame\n",
    "for ticker in valid_tickers:\n",
    "    try:\n",
    "        # Fetch the current data for the ticker\n",
    "        stock_data = quandl.get(f\"WIKI/{ticker}\", rows=1)\n",
    "        stock_data['Ticker'] = ticker\n",
    "        data_frames.append(stock_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "\n",
    "# Concatenate all the individual DataFrames into one\n",
    "if data_frames:\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "    # Display the collected data\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"No valid data fetched.\")\n",
    "\n",
    "# Check if the data was collected and concatenate it into a DataFrame\n",
    "if data_frames:\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "    # Display the collected data\n",
    "    print(data)\n",
    "    data.to_csv('Collected_Stock_Data.csv', index=False)\n",
    "else:\n",
    "    print(\"No valid data fetched.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>------------------ARDA ADJUSTMENT------------------<h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching metadata: This call exceeds the amount of data that quandl.get_table() allows.         Please use the following link in your browser, which will download the full results as         a CSV file: https://www.quandl.com/api/v3/datatables/WIKI/PRICES?qopts.export=true&api_key=xyrEL2EBKf8XvGzp1YdA . See         our API documentation for more info:         https://docs.quandl.com/docs/in-depth-usage-1#section-download-an-entire-table\n",
      "No stocks found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import quandl\n",
    "\n",
    "# Set the API key\n",
    "quandl.ApiConfig.api_key = \"xyrEL2EBKf8XvGzp1YdA\"\n",
    "\n",
    "# Function to check if a ticker has a split ratio other than 1\n",
    "def get_data_with_split_ratio(ticker):\n",
    "    try:\n",
    "        # Fetch data for the ticker\n",
    "        stock_data = quandl.get(f\"WIKI/{ticker}\")\n",
    "        # Filter to keep only the columns of interest\n",
    "        stock_data = stock_data[['Adj. Close', 'Split Ratio']]\n",
    "        stock_data['Ticker'] = ticker\n",
    "        return stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Prepare an empty DataFrame to store the data\n",
    "data_frames = []\n",
    "\n",
    "# Function to fetch tickers in chunks and process them\n",
    "def fetch_and_process_tickers(chunk_size=10, max_rows=10):\n",
    "    try:\n",
    "        start = 0\n",
    "        while len(data_frames) < max_rows:\n",
    "            # Fetch the list of tickers in chunks\n",
    "            metadata = quandl.get_table('WIKI/PRICES', qopts={'columns': ['ticker']}, paginate=True)\n",
    "            tickers = metadata['ticker'].unique()[start:start + chunk_size]\n",
    "            start += chunk_size\n",
    "            \n",
    "            # Fetch the data for each ticker and store it in the DataFrame\n",
    "            for ticker in tickers:\n",
    "                data = get_data_with_split_ratio(ticker)\n",
    "                if data is not None:\n",
    "                    data_frames.append(data)\n",
    "                # Stop if we have reached the desired number of rows\n",
    "                if len(data_frames) >= max_rows:\n",
    "                    break\n",
    "            # Break outer loop if we have reached the desired number of rows\n",
    "            if len(data_frames) >= max_rows:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metadata: {e}\")\n",
    "\n",
    "# Fetch and process tickers\n",
    "fetch_and_process_tickers()\n",
    "\n",
    "# Concatenate all the individual DataFrames into one\n",
    "if data_frames:\n",
    "    result_data = pd.concat(data_frames, ignore_index=True)\n",
    "    # Limit the result to 1000 rows\n",
    "    result_data = result_data.head(1000)\n",
    "    # Display the collected data\n",
    "    print(result_data)\n",
    "    # Export the collected data to a CSV file\n",
    "    result_data.to_csv('Stocks_With_Split_Ratios.csv', index=False)\n",
    "else:\n",
    "    print(\"No stocks found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['ticker', 'date', 'open', 'high', 'low', 'close', 'volume', 'ex-dividend', 'split_ratio', 'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Set the API key\n",
    "api_key = \"xyrEL2EBKf8XvGzp1YdA\"\n",
    "\n",
    "# Define the request URL for the WIKI dataset to get column names\n",
    "url = f\"https://data.nasdaq.com/api/v3/datatables/WIKI/PRICES.json?qopts.columns=ticker,date,open,high,low,close,volume,ex-dividend,split_ratio,adj_open,adj_high,adj_low,adj_close,adj_volume&api_key={api_key}\"\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Extract and print the columns\n",
    "    columns = data['datatable']['columns']\n",
    "    column_names = [column['name'] for column in columns]\n",
    "    print(\"Available columns:\", column_names)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data. Status code: 500\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL of the webpage that contains stock split data\n",
    "url = 'https://example.com/stock-splits'  # Replace with the actual URL\n",
    "\n",
    "# Custom headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Making the GET request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Adjust the class names and tags based on the actual HTML structure\n",
    "    splits_data = soup.find_all('div', class_='split-data-class')\n",
    "    for split in splits_data:\n",
    "        print(split.text)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
